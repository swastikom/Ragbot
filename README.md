# Local LLM and RAG Setup

This project demonstrates how to set up a local large language model (LLM) and a Retrieval-Augmented Generation (RAG) system using Ollama and FastAPI.

---

### üõ†Ô∏è Getting Started

Follow these steps to set up the project and get the server running.

#### 1. Install Dependencies

First, you'll need to install the required Python libraries from the `requirements.txt` file.

```bash
pip install -r requirements.txt
