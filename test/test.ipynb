{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfd9959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d07f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports for using Ollama\n",
    "from langchain_ollama import OllamaEmbeddings # Ollama Embeddings\n",
    "from langchain_ollama import ChatOllama # Ollama LLM Model Holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8502a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "llm = ChatOllama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326ad923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_location = \"./chroma_langchain_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3478d541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database not found. Running initial setup...\n",
      "Documents added to Chroma DB.\n"
     ]
    }
   ],
   "source": [
    "# Dividing Document into chunks and storing in vector database\n",
    "\n",
    "if not os.path.exists(db_location):\n",
    "    print(\"Database not found. Running initial setup...\")\n",
    "    loader = PyPDFLoader(r\"D:\\\\Ragbot\\\\test\\\\Agni.pdf\") # PDF File path\n",
    "    pages = loader.load_and_split()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=db_location)\n",
    "    print(\"Documents added to Chroma DB.\")\n",
    "    exit(\"Please create the Chroma database first by running the setup code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8197e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the initial database and initializes retriever\n",
    "vector_store = Chroma(persist_directory=db_location, embedding_function=embeddings)\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ea2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 1: To rephrase the user's question based on history\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7450c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 2: To answer the question using the retrieved context\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9408c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain combined with history and qa chain\n",
    "\n",
    "rag_chain_with_history = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c227002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Answer with History\n",
    "\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "def ask_with_history(query: str):\n",
    "    global chat_history\n",
    "    \n",
    "    if query.lower() == \"clear history\":\n",
    "        chat_history.clear()\n",
    "        print(\"Chat history cleared. Ask me a new question.\")\n",
    "        return\n",
    "    \n",
    "    result = rag_chain_with_history.invoke({\n",
    "        \"input\": query,\n",
    "        \"chat_history\": chat_history.messages\n",
    "    })\n",
    "    print(result[\"answer\"])\n",
    "    \n",
    "    chat_history.add_user_message(query)\n",
    "    chat_history.add_ai_message(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ccbffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt to clear history\n",
    "# ask_with_history(\"clear history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bce4b158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agni is the Vedic god of fire, revered as a divine presence that bridges the human and divine realms. He is also considered the principle of divine communication, transformation, and continuity, symbolizing energy, passion, and life force. Agni is not just a god of fire but an essential mediator of ritual.\n"
     ]
    }
   ],
   "source": [
    "# Example Question\n",
    "ask_with_history(\"Who is Agni?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43be2bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked \"Who is Agni?\" and I provided a response. However, I should have been more precise in my answer. You essentially asked \"Who\" or \"What is\" Agni, rather than asking about his characteristics or role. My initial response was an attempt to provide more context, but I can try again with a simpler answer if you'd like.\n"
     ]
    }
   ],
   "source": [
    "# Example Question to demonstrate chatting with history\n",
    "ask_with_history(\"What did I just asked?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00c50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history cleared. Ask me a new question.\n"
     ]
    }
   ],
   "source": [
    "# Example prompt to clear history\n",
    "ask_with_history(\"clear history\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f2b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, you didn't ask anything about Agni. This conversation just started with the context provided about Agni.\n"
     ]
    }
   ],
   "source": [
    "# Example Question to demonstrate chatting with history\n",
    "# In this case after clearing the chat history\n",
    "ask_with_history(\"Did I asked something related to Agni?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
